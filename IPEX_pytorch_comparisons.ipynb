{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvCR3kBrz2Lz",
        "outputId": "15359581-087d-48c0-faae-2a0b0f509093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 5.118703842163086\n",
            "Epoch 2/20, Loss: 5.061988353729248\n",
            "Epoch 3/20, Loss: 2.975680112838745\n",
            "Epoch 4/20, Loss: 2.0558178424835205\n",
            "Epoch 5/20, Loss: 1.207516074180603\n",
            "Epoch 6/20, Loss: 0.9351003170013428\n",
            "Epoch 7/20, Loss: 0.6878919005393982\n",
            "Epoch 8/20, Loss: 0.4367513656616211\n",
            "Epoch 9/20, Loss: 0.3448368310928345\n",
            "Epoch 10/20, Loss: 0.2254834622144699\n",
            "Epoch 11/20, Loss: 0.13153383135795593\n",
            "Epoch 12/20, Loss: 0.11354824900627136\n",
            "Epoch 13/20, Loss: 0.10814617574214935\n",
            "Epoch 14/20, Loss: 0.05989956110715866\n",
            "Epoch 15/20, Loss: 0.03741941601037979\n",
            "Epoch 16/20, Loss: 0.11595889180898666\n",
            "Epoch 17/20, Loss: 0.03822680190205574\n",
            "Epoch 18/20, Loss: 0.0908135250210762\n",
            "Epoch 19/20, Loss: 0.05846743285655975\n",
            "Epoch 20/20, Loss: 0.12713192403316498\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "\n",
        "# Define a custom dataset class for generating poems\n",
        "class PoemDataset(Dataset):\n",
        "    def __init__(self, poems, tokenizer, max_length=100):\n",
        "        self.poems = poems\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.poems)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        poem = self.poems[idx]\n",
        "        input_ids = self.tokenizer.encode(poem, max_length=self.max_length, truncation=True)\n",
        "        return torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "# Load poems from a .txt file\n",
        "def load_poems_from_txt(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        poems = file.read().split('\\n\\n')  # Assuming poems are separated by empty lines\n",
        "    return poems\n",
        "\n",
        "# Define path to the .txt file containing poems\n",
        "txt_file_path = 'poem.txt'\n",
        "\n",
        "# Load poems from the .txt file\n",
        "poems = load_poems_from_txt(txt_file_path)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 20\n",
        "batch_size = 4\n",
        "max_length = 200\n",
        "\n",
        "# Create dataset and data loader\n",
        "dataset = PoemDataset(poems, tokenizer, max_length=max_length)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch.to(model.device)\n",
        "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Save trained model checkpoint\n",
        "torch.save(model.state_dict(), 'poem_generator_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install modin\n",
        "!pip uninstall torch\n",
        "!pip install torch==1.13.1\n",
        "!pip install intel_extension_for_pytorch==1.13.100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "na1DsRaYz_n1",
        "outputId": "01fa2bfa-0ba3-42f7-dea9-de3bc351528e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Collecting modin\n",
            "  Downloading modin-0.26.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<2.2,>=2.1 (from modin)\n",
            "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from modin) (23.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from modin) (1.23.5)\n",
            "Requirement already satisfied: fsspec>=2022.05.0 in /usr/local/lib/python3.10/dist-packages (from modin) (2023.6.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from modin) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2,>=2.1->modin) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2,>=2.1->modin) (2023.4)\n",
            "Collecting tzdata>=2022.1 (from pandas<2.2,>=2.1->modin)\n",
            "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2,>=2.1->modin) (1.16.0)\n",
            "Installing collected packages: tzdata, pandas, modin\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "bigframes 0.20.1 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed modin-0.26.1 pandas-2.1.4 tzdata-2023.4\n",
            "Found existing installation: torch 2.1.0+cu121\n",
            "Uninstalling torch-2.1.0+cu121:\n",
            "  Would remove:\n",
            "    /usr/local/bin/convert-caffe2-to-onnx\n",
            "    /usr/local/bin/convert-onnx-to-caffe2\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.10/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/nvfuser/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch-2.1.0+cu121.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.10/dist-packages/torchgen/*\n",
            "Proceed (Y/n)? Y\n",
            "y\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m662.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.9.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.42.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting intel_extension_for_pytorch==1.13.100\n",
            "  Downloading intel_extension_for_pytorch-1.13.100-cp310-cp310-manylinux2014_x86_64.whl (38.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from intel_extension_for_pytorch==1.13.100) (5.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from intel_extension_for_pytorch==1.13.100) (1.23.5)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: intel_extension_for_pytorch\n",
            "Successfully installed intel_extension_for_pytorch-1.13.100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch\n",
        "!pip install torch==1.13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "37df2RHI1xOK",
        "outputId": "7187ea67-376c-4572-ba96-2d423fbfa56e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch==1.13\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13) (4.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13) (0.42.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training with/without IPEX (Intel Extension for pytorch)**"
      ],
      "metadata": {
        "id": "FXTy-JDF5eXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without IPEX"
      ],
      "metadata": {
        "id": "xqohPyg35ylE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Define the model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Generate random input and output data\n",
        "x_train = torch.randn(1000, input_size)\n",
        "y_train = torch.randn(1000, output_size)\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with optim\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        inputs = x_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Optim Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Total training time\n",
        "print(f\"Total Training Time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Y60c_G4vj5",
        "outputId": "27e37a6d-4ada-4de1-c324-80a0e5139954"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optim Epoch [1/10], Loss: 2.0920748710632324\n",
            "Optim Epoch [2/10], Loss: 1.9652900695800781\n",
            "Optim Epoch [3/10], Loss: 1.853972315788269\n",
            "Optim Epoch [4/10], Loss: 1.7561496496200562\n",
            "Optim Epoch [5/10], Loss: 1.6701105833053589\n",
            "Optim Epoch [6/10], Loss: 1.5943677425384521\n",
            "Optim Epoch [7/10], Loss: 1.5276281833648682\n",
            "Optim Epoch [8/10], Loss: 1.4687671661376953\n",
            "Optim Epoch [9/10], Loss: 1.4168058633804321\n",
            "Optim Epoch [10/10], Loss: 1.3708926439285278\n",
            "Total Training Time: 0.40898799896240234 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Define the model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Generate random input and output data\n",
        "x_train = torch.randn(1000, input_size)\n",
        "y_train = torch.randn(1000, output_size)\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with optim\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        inputs = x_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Optim Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Total training time\n",
        "print(f\"Total Training Time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRgg4Y_157jZ",
        "outputId": "0aa60971-05ea-495a-c96f-705e79fdbbc9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optim Epoch [1/20], Loss: 0.9241153001785278\n",
            "Optim Epoch [2/20], Loss: 0.8671150803565979\n",
            "Optim Epoch [3/20], Loss: 0.8195785284042358\n",
            "Optim Epoch [4/20], Loss: 0.7801409959793091\n",
            "Optim Epoch [5/20], Loss: 0.7476246953010559\n",
            "Optim Epoch [6/20], Loss: 0.7210130095481873\n",
            "Optim Epoch [7/20], Loss: 0.6994288563728333\n",
            "Optim Epoch [8/20], Loss: 0.6821167469024658\n",
            "Optim Epoch [9/20], Loss: 0.6684258580207825\n",
            "Optim Epoch [10/20], Loss: 0.6577962040901184\n",
            "Optim Epoch [11/20], Loss: 0.6497465968132019\n",
            "Optim Epoch [12/20], Loss: 0.6438636183738708\n",
            "Optim Epoch [13/20], Loss: 0.639792799949646\n",
            "Optim Epoch [14/20], Loss: 0.637230396270752\n",
            "Optim Epoch [15/20], Loss: 0.6359168291091919\n",
            "Optim Epoch [16/20], Loss: 0.6356302499771118\n",
            "Optim Epoch [17/20], Loss: 0.6361813545227051\n",
            "Optim Epoch [18/20], Loss: 0.6374095678329468\n",
            "Optim Epoch [19/20], Loss: 0.6391780376434326\n",
            "Optim Epoch [20/20], Loss: 0.6413711309432983\n",
            "Total Training Time: 0.6625721454620361 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "# Define the model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 40\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Generate random input and output data\n",
        "x_train = torch.randn(1000, input_size)\n",
        "y_train = torch.randn(1000, output_size)\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with optim\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        inputs = x_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Optim Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Total training time\n",
        "print(f\"Total Training Time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC6llUAP6RyK",
        "outputId": "76500133-3d5a-4c7c-d5c9-6d0c713310bd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optim Epoch [1/40], Loss: 0.86285400390625\n",
            "Optim Epoch [2/40], Loss: 0.8515642881393433\n",
            "Optim Epoch [3/40], Loss: 0.8436882495880127\n",
            "Optim Epoch [4/40], Loss: 0.8386602401733398\n",
            "Optim Epoch [5/40], Loss: 0.835996150970459\n",
            "Optim Epoch [6/40], Loss: 0.8352826833724976\n",
            "Optim Epoch [7/40], Loss: 0.8361666798591614\n",
            "Optim Epoch [8/40], Loss: 0.8383474946022034\n",
            "Optim Epoch [9/40], Loss: 0.8415696024894714\n",
            "Optim Epoch [10/40], Loss: 0.8456161618232727\n",
            "Optim Epoch [11/40], Loss: 0.8503037095069885\n",
            "Optim Epoch [12/40], Loss: 0.855477511882782\n",
            "Optim Epoch [13/40], Loss: 0.8610072731971741\n",
            "Optim Epoch [14/40], Loss: 0.866784393787384\n",
            "Optim Epoch [15/40], Loss: 0.8727173805236816\n",
            "Optim Epoch [16/40], Loss: 0.8787308931350708\n",
            "Optim Epoch [17/40], Loss: 0.8847628831863403\n",
            "Optim Epoch [18/40], Loss: 0.8907622694969177\n",
            "Optim Epoch [19/40], Loss: 0.8966876268386841\n",
            "Optim Epoch [20/40], Loss: 0.9025061130523682\n",
            "Optim Epoch [21/40], Loss: 0.9081912636756897\n",
            "Optim Epoch [22/40], Loss: 0.9137226343154907\n",
            "Optim Epoch [23/40], Loss: 0.9190850257873535\n",
            "Optim Epoch [24/40], Loss: 0.9242672920227051\n",
            "Optim Epoch [25/40], Loss: 0.9292614459991455\n",
            "Optim Epoch [26/40], Loss: 0.934062659740448\n",
            "Optim Epoch [27/40], Loss: 0.9386684894561768\n",
            "Optim Epoch [28/40], Loss: 0.943078875541687\n",
            "Optim Epoch [29/40], Loss: 0.9472944736480713\n",
            "Optim Epoch [30/40], Loss: 0.9513182640075684\n",
            "Optim Epoch [31/40], Loss: 0.9551532864570618\n",
            "Optim Epoch [32/40], Loss: 0.9588044285774231\n",
            "Optim Epoch [33/40], Loss: 0.9622763395309448\n",
            "Optim Epoch [34/40], Loss: 0.9655748009681702\n",
            "Optim Epoch [35/40], Loss: 0.9687058329582214\n",
            "Optim Epoch [36/40], Loss: 0.9716750383377075\n",
            "Optim Epoch [37/40], Loss: 0.9744893908500671\n",
            "Optim Epoch [38/40], Loss: 0.9771544933319092\n",
            "Optim Epoch [39/40], Loss: 0.9796772599220276\n",
            "Optim Epoch [40/40], Loss: 0.9820637702941895\n",
            "Total Training Time: 0.5762951374053955 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Ipex"
      ],
      "metadata": {
        "id": "anOneeth54Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import intel_extension_for_pytorch as ipex\n",
        "\n",
        "# Define the model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Generate random input and output data\n",
        "x_train = torch.randn(1000, input_size)\n",
        "y_train = torch.randn(1000, output_size)\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Optimize the model with IPEX, passing the optimizer\n",
        "model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        inputs = x_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Optim Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Total training time\n",
        "print(f\"Total Training Time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwKxJaxn5SEn",
        "outputId": "3b70c075-ad59-4799-b3ee-13760ea5f143"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optim Epoch [1/10], Loss: 0.8854290246963501\n",
            "Optim Epoch [2/10], Loss: 0.8616810441017151\n",
            "Optim Epoch [3/10], Loss: 0.8404115438461304\n",
            "Optim Epoch [4/10], Loss: 0.821343183517456\n",
            "Optim Epoch [5/10], Loss: 0.8042304515838623\n",
            "Optim Epoch [6/10], Loss: 0.7888562083244324\n",
            "Optim Epoch [7/10], Loss: 0.7750279307365417\n",
            "Optim Epoch [8/10], Loss: 0.7625757455825806\n",
            "Optim Epoch [9/10], Loss: 0.7513487935066223\n",
            "Optim Epoch [10/10], Loss: 0.7412134408950806\n",
            "Total Training Time: 0.15320825576782227 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import intel_extension_for_pytorch as ipex\n",
        "\n",
        "# Define the model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Generate random input and output data\n",
        "x_train = torch.randn(1000, input_size)\n",
        "y_train = torch.randn(1000, output_size)\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Optimize the model with IPEX, passing the optimizer\n",
        "model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        inputs = x_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Optim Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Total training time\n",
        "print(f\"Total Training Time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fK_dR95_QY",
        "outputId": "3a54037f-fe41-4aa2-b883-43c868b990fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optim Epoch [1/20], Loss: 2.200347423553467\n",
            "Optim Epoch [2/20], Loss: 2.1515400409698486\n",
            "Optim Epoch [3/20], Loss: 2.107692003250122\n",
            "Optim Epoch [4/20], Loss: 2.068255662918091\n",
            "Optim Epoch [5/20], Loss: 2.032745838165283\n",
            "Optim Epoch [6/20], Loss: 2.0007340908050537\n",
            "Optim Epoch [7/20], Loss: 1.9718413352966309\n",
            "Optim Epoch [8/20], Loss: 1.9457323551177979\n",
            "Optim Epoch [9/20], Loss: 1.9221101999282837\n",
            "Optim Epoch [10/20], Loss: 1.9007114171981812\n",
            "Optim Epoch [11/20], Loss: 1.8813029527664185\n",
            "Optim Epoch [12/20], Loss: 1.863677978515625\n",
            "Optim Epoch [13/20], Loss: 1.8476520776748657\n",
            "Optim Epoch [14/20], Loss: 1.8330620527267456\n",
            "Optim Epoch [15/20], Loss: 1.819762945175171\n",
            "Optim Epoch [16/20], Loss: 1.8076251745224\n",
            "Optim Epoch [17/20], Loss: 1.7965340614318848\n",
            "Optim Epoch [18/20], Loss: 1.7863866090774536\n",
            "Optim Epoch [19/20], Loss: 1.7770910263061523\n",
            "Optim Epoch [20/20], Loss: 1.7685662508010864\n",
            "Total Training Time: 0.2639493942260742 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import intel_extension_for_pytorch as ipex\n",
        "\n",
        "# Define the model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define training parameters\n",
        "num_epochs = 40\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "input_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Generate random input and output data\n",
        "x_train = torch.randn(1000, input_size)\n",
        "y_train = torch.randn(1000, output_size)\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Optimize the model with IPEX, passing the optimizer\n",
        "model, optimizer = ipex.optimize(model, optimizer=optimizer)\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        inputs = x_train[i:i+batch_size]\n",
        "        targets = y_train[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Optim Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Total training time\n",
        "print(f\"Total Training Time: {time.time() - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-uzQJG06ATR",
        "outputId": "d20cd29b-c855-4468-ce4e-6769adebc802"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optim Epoch [1/40], Loss: 1.0372893810272217\n",
            "Optim Epoch [2/40], Loss: 0.9968695640563965\n",
            "Optim Epoch [3/40], Loss: 0.9627225995063782\n",
            "Optim Epoch [4/40], Loss: 0.9339970350265503\n",
            "Optim Epoch [5/40], Loss: 0.9099512696266174\n",
            "Optim Epoch [6/40], Loss: 0.8899393081665039\n",
            "Optim Epoch [7/40], Loss: 0.8733994364738464\n",
            "Optim Epoch [8/40], Loss: 0.8598429560661316\n",
            "Optim Epoch [9/40], Loss: 0.8488450050354004\n",
            "Optim Epoch [10/40], Loss: 0.8400364518165588\n",
            "Optim Epoch [11/40], Loss: 0.8330973982810974\n",
            "Optim Epoch [12/40], Loss: 0.827750027179718\n",
            "Optim Epoch [13/40], Loss: 0.8237534761428833\n",
            "Optim Epoch [14/40], Loss: 0.820899486541748\n",
            "Optim Epoch [15/40], Loss: 0.8190079927444458\n",
            "Optim Epoch [16/40], Loss: 0.817923367023468\n",
            "Optim Epoch [17/40], Loss: 0.8175113797187805\n",
            "Optim Epoch [18/40], Loss: 0.8176562786102295\n",
            "Optim Epoch [19/40], Loss: 0.8182589411735535\n",
            "Optim Epoch [20/40], Loss: 0.819233775138855\n",
            "Optim Epoch [21/40], Loss: 0.820507824420929\n",
            "Optim Epoch [22/40], Loss: 0.8220189213752747\n",
            "Optim Epoch [23/40], Loss: 0.8237134218215942\n",
            "Optim Epoch [24/40], Loss: 0.8255464434623718\n",
            "Optim Epoch [25/40], Loss: 0.8274796605110168\n",
            "Optim Epoch [26/40], Loss: 0.8294806480407715\n",
            "Optim Epoch [27/40], Loss: 0.831522524356842\n",
            "Optim Epoch [28/40], Loss: 0.8335822820663452\n",
            "Optim Epoch [29/40], Loss: 0.8356415033340454\n",
            "Optim Epoch [30/40], Loss: 0.8376846313476562\n",
            "Optim Epoch [31/40], Loss: 0.8396989107131958\n",
            "Optim Epoch [32/40], Loss: 0.8416743874549866\n",
            "Optim Epoch [33/40], Loss: 0.8436027765274048\n",
            "Optim Epoch [34/40], Loss: 0.845477819442749\n",
            "Optim Epoch [35/40], Loss: 0.847294807434082\n",
            "Optim Epoch [36/40], Loss: 0.8490501046180725\n",
            "Optim Epoch [37/40], Loss: 0.8507413268089294\n",
            "Optim Epoch [38/40], Loss: 0.8523669242858887\n",
            "Optim Epoch [39/40], Loss: 0.8539263010025024\n",
            "Optim Epoch [40/40], Loss: 0.8554191589355469\n",
            "Total Training Time: 0.3940904140472412 seconds\n"
          ]
        }
      ]
    }
  ]
}